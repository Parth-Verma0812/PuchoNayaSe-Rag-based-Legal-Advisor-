{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44f6386-6f23-4f22-b39a-6358146ac37a",
   "metadata": {},
   "source": [
    "# PuchoNayaSe : Rag Based Legal Advisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3280b4-1590-4720-9988-5c7ca8c8c820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.26)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.93.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: PyMuPDF in /opt/anaconda3/lib/python3.12/site-packages (1.26.3)\n",
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai faiss-cpu tiktoken PyMuPDF streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093b6f0-c10a-4d0c-90eb-1cc8be2a76d4",
   "metadata": {},
   "source": [
    "* Langchain : Framework for chainning LLMs and retrivers\n",
    "* openai : OpenAI API for GPT-3.5/GPT-4\n",
    "* faiss-cpu : Vector store for efficient similarity search\n",
    "* tiktoken : Tokenizer for OpenAI models\n",
    "* PyMuPDF : For extracting text from PDF files\n",
    "* streamlit : for frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21d0fc-6813-4703-ad45-be1f911a5da8",
   "metadata": {},
   "source": [
    "# Extracting text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d000b9-f273-4f75-bd7b-e4b43e9ce43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # from PyMuPDF\n",
    "import os\n",
    "# Legal ADVISIOR/\n",
    "def extract_text_from_pdf_file(folder_path):\n",
    "    text_=[]\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            doc=fitz.open(os.path.join(folder_path,filename))\n",
    "            text=\"\"\n",
    "            for page in doc:\n",
    "                text+=page.get_text()\n",
    "            text_.append(text)\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0793a-fa6f-419f-bf43-b4149a33091b",
   "metadata": {},
   "source": [
    "* This function reads the text from pdf files\n",
    "* fitz extracts the plain text from pdf files by using get_text function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0617de-0f12-490e-818c-e23f27cce596",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2a9008-59d3-444c-9005-eec4dbce87d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of Chunks Created : 3159 \n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# loading textual data\n",
    "texts=extract_text_from_pdf_file(\"/Users/parthverma/Desktop/Programming/Legal ADVISIOR\")\n",
    "\n",
    "# making chunks of textual data\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=800,chunk_overlap=100)\n",
    "\n",
    "chunks=[]\n",
    "for doc in texts:\n",
    "    chunks.extend(splitter.split_text(doc))\n",
    "print(f\"total number of Chunks Created : {len(chunks)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df854be-61b7-4782-8488-831499e04e47",
   "metadata": {},
   "source": [
    "* LangChain’s RecursiveCharacterTextSplitter splits large texts into overlapping chunks : This helps avoid LLM context size issues and ensures continuity.\n",
    "\n",
    "* Chunk_size=800 : max token-like length\n",
    "\n",
    "* chunk_overlap=100 : small overlap for context continuity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af81057-3f0d-440c-8a04-f31522958724",
   "metadata": {},
   "source": [
    "# Embedd chunks and store in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2337d88d-cea8-4df4-9f41-ddf42fecdad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/1bdd_5055y93jwnnyhkcbndh0000gn/T/ipykernel_7626/851392049.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3159/3159 [00:00<00:00, 2133280.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load huggingface embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the FAISS vector store\n",
    "db = FAISS.from_texts(list(tqdm(chunks)), embedding_model)\n",
    "\n",
    "# Save the vector store to disk\n",
    "db.save_local(\"legal_faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5656ff98-f193-45eb-b9a0-b78fe53569d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c869d-fdec-499f-8614-7d2be6206572",
   "metadata": {},
   "source": [
    "# Loading Vectore Store and Building Rag Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4763750a-493d-427f-ad8a-9fec06f95dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Reloading embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the saved FAISS index\n",
    "vectordb = FAISS.load_local(\"legal_faiss_index\", embedding_model,allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefff025-f438-4172-845b-c994fb0604b4",
   "metadata": {},
   "source": [
    "# Build a Retrieval-Based QA Chain (Without OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd9741b-157a-4460-b4e4-0d81d2bb5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Loading local model for text generation\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", tokenizer=\"google/flan-t5-base\")\n",
    "\n",
    "# Creating retriever from FAISS\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Custom RAG function\n",
    "def rag_local_qa(query, retriever, qa_pipeline):\n",
    "    # Step 1: Retrieve top relevant documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Step 2: Combine contents\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Step 3: Prepare prompt\n",
    "    prompt = f\"Answer the question based on the following context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    # Step 4: Generate answer\n",
    "    result = qa_pipeline(prompt, max_length=256, do_sample=False)[0]['generated_text']\n",
    "\n",
    "    return result, docs  # answer + source docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6d765-fc7b-49a1-a285-ce7115166263",
   "metadata": {},
   "source": [
    "# Asking Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e6d42cf-5189-4c34-8b4d-487cb6e535cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/1bdd_5055y93jwnnyhkcbndh0000gn/T/ipykernel_7626/4266166673.py:12: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1002 > 512). Running this sequence through the model will result in indexing errors\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1[imprisonment for life], and shall also be liable to fine\n",
      "\n",
      "--- Source 1 ---\n",
      "intended.—If a person, by doing anything which he intends or knows to be likely to cause death, \n",
      "commits culpable homicide by causing the death of any person, whose death he neither intends nor knows \n",
      "himself to be likely to cause, the culpable homicide committed by the offender is of the descriptio\n",
      "\n",
      "--- Source 2 ---\n",
      "years, and shall also be liable to fine. \n",
      "307. Attempt to murder.—Whoever does any act with such intention or knowledge, and under such \n",
      "circumstances that, if he by that act caused death, he would be guilty of murder, shall be punished with \n",
      "imprisonment of either description for a term which may e\n",
      "\n",
      "--- Source 3 ---\n",
      "by such act, shall be punished with imprisonment of either description for a term which may extend to \n",
      "seven years, or with fine, or with both. \n",
      "Illustration \n",
      "A, on grave and sudden provocation, fires a pistol at Z, under such circumstances that if he thereby caused death he would \n",
      "be guilty of culp\n",
      "\n",
      "--- Source 4 ---\n",
      "395. Punishment for dacoity.—Whoever commits dacoity shall be punished with 1[imprisonment for \n",
      "life], or with rigorous imprisonment for a term which may extend to ten years, and shall also be liable to \n",
      "fine. \n",
      "396. Dacoity with murder.—If any one of five or more persons, who are conjointly committi\n",
      "\n",
      "--- Source 5 ---\n",
      "CHAPTER XVI \n",
      "OF OFFENCES AFFECTING THE HUMAN BODY \n",
      " \n",
      "Of offences affecting life \n",
      "299. Culpable homicide. \n",
      "300. Murder. \n",
      "When culpable homicide is not murder. \n",
      "301. Culpable homicide by causing death of person other than person whose death was intended. \n",
      "302. Punishment for murder. \n",
      "303. Punishment f\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the punishment for theft under IPC?\"\n",
    "query1=\"What is the punishment for murder under IPC?\"\n",
    "answer, sources = rag_local_qa(query1, retriever, qa_pipeline)\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\", answer)\n",
    "\n",
    "# Print sources\n",
    "for i, doc in enumerate(sources):\n",
    "    print(f\"\\n--- Source {i+1} ---\")\n",
    "    print(doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9bd059a-9aa7-4f45-aa81-f51078b52af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/var/folders/jg/1bdd_5055y93jwnnyhkcbndh0000gn/T/ipykernel_7626/3453157062.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Loads vector DB and retriever\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectordb = FAISS.load_local(\"legal_faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Loads the LLM\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Builds QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba193d7-3c91-472d-8428-d7ece6252c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://64c7f0743013d75c4b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://64c7f0743013d75c4b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines chatbot logic\n",
    "def legal_qa_bot(message):\n",
    "    try:\n",
    "        result = qa_chain(message)\n",
    "        return result[\"result\"]\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {str(e)}\"\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=legal_qa_bot,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask your legal question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"PuchoNyaySe : Legal Advisor 👨🏻‍⚖️\",\n",
    "    description=\"\"\"Get Instant Legal Answers from Indian Law 📚\n",
    "Ask clear, simple legal questions — no lawyer required!\"\"\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce9b46-ab66-43b3-8296-ba23c7ca6264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
